{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T04:24:09.926854Z",
     "iopub.status.busy": "2025-05-19T04:24:09.926702Z",
     "iopub.status.idle": "2025-05-19T04:24:19.457563Z",
     "shell.execute_reply": "2025-05-19T04:24:19.457012Z",
     "shell.execute_reply.started": "2025-05-19T04:24:09.926839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m015\u001b[0m (\u001b[33mda24m015-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"55f4fea427568f21b05f76ecad678f380953242f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T10:03:45.351038Z",
     "iopub.status.busy": "2025-05-19T10:03:45.350279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: olaafvbq\n",
      "Sweep URL: https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dqsxk6ls with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.009774819477897757\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_100351-dqsxk6ls</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/dqsxk6ls' target=\"_blank\">zesty-sweep-1</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/dqsxk6ls' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/dqsxk6ls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▇██▇▇████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▃▇▁▆█▃▂▅█▅</td></tr><tr><td>val_loss</td><td>▇▁█▅▅▅▆▇▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.66739</td></tr><tr><td>train_loss</td><td>1.09943</td></tr><tr><td>val_accuracy</td><td>0.58549</td></tr><tr><td>val_loss</td><td>1.42533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:128_hid:128_layers:1</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/dqsxk6ls' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/dqsxk6ls</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_100351-dqsxk6ls/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hhe5g3qf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007335588748686146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_101140-hhe5g3qf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/hhe5g3qf' target=\"_blank\">fearless-sweep-2</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/hhe5g3qf' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/hhe5g3qf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▇█████▇█▇</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▂▁▂</td></tr><tr><td>val_accuracy</td><td>▁▅▅▄▅▇▄█▄▁</td></tr><tr><td>val_loss</td><td>▄▁▃▅▂▅▆▂▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.69899</td></tr><tr><td>train_loss</td><td>0.99105</td></tr><tr><td>val_accuracy</td><td>0.60141</td></tr><tr><td>val_loss</td><td>1.3677</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:128_hid:128_layers:1</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/hhe5g3qf' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/hhe5g3qf</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_101140-hhe5g3qf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jkx5vswf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00936730552390937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_101938-jkx5vswf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/jkx5vswf' target=\"_blank\">royal-sweep-3</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/jkx5vswf' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/jkx5vswf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇█▇▇█▇▇</td></tr><tr><td>val_loss</td><td>█▅▄▃▁▃▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.76157</td></tr><tr><td>train_loss</td><td>0.76827</td></tr><tr><td>val_accuracy</td><td>0.68124</td></tr><tr><td>val_loss</td><td>1.09354</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/jkx5vswf' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/jkx5vswf</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_101938-jkx5vswf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mn8rybzc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008973760551983036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_102857-mn8rybzc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/mn8rybzc' target=\"_blank\">absurd-sweep-4</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/mn8rybzc' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/mn8rybzc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▆▇▇▇█▇</td></tr><tr><td>val_loss</td><td>█▄▇▃▁▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.76983</td></tr><tr><td>train_loss</td><td>0.7418</td></tr><tr><td>val_accuracy</td><td>0.68305</td></tr><tr><td>val_loss</td><td>1.09754</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/mn8rybzc' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/mn8rybzc</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_102857-mn8rybzc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aa0dix67 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004783152378480684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_103817-aa0dix67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/aa0dix67' target=\"_blank\">rosy-sweep-5</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/aa0dix67' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/aa0dix67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79318</td></tr><tr><td>train_loss</td><td>0.66838</td></tr><tr><td>val_accuracy</td><td>0.71097</td></tr><tr><td>val_loss</td><td>1.00625</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/aa0dix67' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/aa0dix67</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_103817-aa0dix67/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: clic769s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0011981221201781225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_104333-clic769s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/clic769s' target=\"_blank\">polar-sweep-6</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/clic769s' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/clic769s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▃▂▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.78579</td></tr><tr><td>train_loss</td><td>0.69596</td></tr><tr><td>val_accuracy</td><td>0.70867</td></tr><tr><td>val_loss</td><td>1.0193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/clic769s' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/clic769s</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_104333-clic769s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ufpaod9v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.006606527216065912\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_104846-ufpaod9v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ufpaod9v' target=\"_blank\">robust-sweep-7</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ufpaod9v' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ufpaod9v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▆▇▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▄▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79378</td></tr><tr><td>train_loss</td><td>0.66885</td></tr><tr><td>val_accuracy</td><td>0.71347</td></tr><tr><td>val_loss</td><td>0.99852</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ufpaod9v' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ufpaod9v</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_104846-ufpaod9v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ev294u9h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005721574064191443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_105402-ev294u9h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ev294u9h' target=\"_blank\">chocolate-sweep-8</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ev294u9h' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ev294u9h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▆▆▇▇▇█▇</td></tr><tr><td>val_loss</td><td>█▄▆▃▂▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79494</td></tr><tr><td>train_loss</td><td>0.66318</td></tr><tr><td>val_accuracy</td><td>0.70794</td></tr><tr><td>val_loss</td><td>1.03412</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ev294u9h' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ev294u9h</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_105402-ev294u9h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b2dpvjfb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006996568826276642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_105915-b2dpvjfb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/b2dpvjfb' target=\"_blank\">young-sweep-9</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/b2dpvjfb' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/b2dpvjfb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.78432</td></tr><tr><td>train_loss</td><td>0.70531</td></tr><tr><td>val_accuracy</td><td>0.7012</td></tr><tr><td>val_loss</td><td>1.04907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/b2dpvjfb' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/b2dpvjfb</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_105915-b2dpvjfb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0r6sgcc7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.009198561973415348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_110428-0r6sgcc7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/0r6sgcc7' target=\"_blank\">worldly-sweep-10</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/0r6sgcc7' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/0r6sgcc7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▇▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▅▂▃▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.77909</td></tr><tr><td>train_loss</td><td>0.71407</td></tr><tr><td>val_accuracy</td><td>0.70536</td></tr><tr><td>val_loss</td><td>1.01782</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/0r6sgcc7' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/0r6sgcc7</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_110428-0r6sgcc7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 07gf6y10 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005890716969271358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_110940-07gf6y10</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/07gf6y10' target=\"_blank\">wandering-sweep-11</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/07gf6y10' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/07gf6y10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▇▅▇▆█▇█</td></tr><tr><td>val_loss</td><td>█▄▃▂▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.78862</td></tr><tr><td>train_loss</td><td>0.6849</td></tr><tr><td>val_accuracy</td><td>0.70687</td></tr><tr><td>val_loss</td><td>1.02893</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/07gf6y10' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/07gf6y10</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_110940-07gf6y10/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7rkokf43 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005342056157149389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_111457-7rkokf43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/7rkokf43' target=\"_blank\">stoic-sweep-12</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/7rkokf43' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/7rkokf43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▆▇▇█▇</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▃▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79864</td></tr><tr><td>train_loss</td><td>0.65349</td></tr><tr><td>val_accuracy</td><td>0.70894</td></tr><tr><td>val_loss</td><td>1.01712</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/7rkokf43' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/7rkokf43</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_111457-7rkokf43/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nhcc9syw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0020793734884825108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_112019-nhcc9syw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/nhcc9syw' target=\"_blank\">absurd-sweep-13</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/nhcc9syw' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/nhcc9syw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79875</td></tr><tr><td>train_loss</td><td>0.65485</td></tr><tr><td>val_accuracy</td><td>0.7151</td></tr><tr><td>val_loss</td><td>0.98045</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/nhcc9syw' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/nhcc9syw</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_112019-nhcc9syw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s024nsj7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0027979344303061576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_112527-s024nsj7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/s024nsj7' target=\"_blank\">legendary-sweep-14</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/s024nsj7' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/s024nsj7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▇▇▆▇▇█</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▄▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80119</td></tr><tr><td>train_loss</td><td>0.64804</td></tr><tr><td>val_accuracy</td><td>0.71638</td></tr><tr><td>val_loss</td><td>0.975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/s024nsj7' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/s024nsj7</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_112527-s024nsj7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xvvpveh3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005592771732945555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_113044-xvvpveh3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/xvvpveh3' target=\"_blank\">wobbly-sweep-15</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/xvvpveh3' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/xvvpveh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▅▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▄▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80921</td></tr><tr><td>train_loss</td><td>0.61959</td></tr><tr><td>val_accuracy</td><td>0.71271</td></tr><tr><td>val_loss</td><td>1.02737</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/xvvpveh3' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/xvvpveh3</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_113044-xvvpveh3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eiudl6b3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.006431619234021762\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_113557-eiudl6b3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/eiudl6b3' target=\"_blank\">gentle-sweep-16</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/eiudl6b3' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/eiudl6b3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79081</td></tr><tr><td>train_loss</td><td>0.67738</td></tr><tr><td>val_accuracy</td><td>0.70699</td></tr><tr><td>val_loss</td><td>1.02895</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/eiudl6b3' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/eiudl6b3</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_113557-eiudl6b3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1o0shizq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.003781289690537184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_114108-1o0shizq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/1o0shizq' target=\"_blank\">graceful-sweep-17</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/1o0shizq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/1o0shizq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▆▆▆▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▄▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79628</td></tr><tr><td>train_loss</td><td>0.65953</td></tr><tr><td>val_accuracy</td><td>0.71376</td></tr><tr><td>val_loss</td><td>1.01374</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/1o0shizq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/1o0shizq</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_114108-1o0shizq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3jyiuhdf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.006716456053333315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_114621-3jyiuhdf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/3jyiuhdf' target=\"_blank\">fast-sweep-18</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/3jyiuhdf' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/3jyiuhdf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▇▇█▇█</td></tr><tr><td>val_loss</td><td>█▅▃▄▃▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.78437</td></tr><tr><td>train_loss</td><td>0.69577</td></tr><tr><td>val_accuracy</td><td>0.70723</td></tr><tr><td>val_loss</td><td>1.02907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/3jyiuhdf' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/3jyiuhdf</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_114621-3jyiuhdf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e0zcp5fh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004078036508399985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_115133-e0zcp5fh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/e0zcp5fh' target=\"_blank\">cosmic-sweep-19</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/e0zcp5fh' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/e0zcp5fh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▃▄▁▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80348</td></tr><tr><td>train_loss</td><td>0.63999</td></tr><tr><td>val_accuracy</td><td>0.71514</td></tr><tr><td>val_loss</td><td>1.00825</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/e0zcp5fh' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/e0zcp5fh</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_115133-e0zcp5fh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bdpwx5o7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005228844670102067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_115640-bdpwx5o7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/bdpwx5o7' target=\"_blank\">vivid-sweep-20</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/bdpwx5o7' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/bdpwx5o7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▅▃▂▂▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80054</td></tr><tr><td>train_loss</td><td>0.64671</td></tr><tr><td>val_accuracy</td><td>0.71971</td></tr><tr><td>val_loss</td><td>0.97426</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/bdpwx5o7' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/bdpwx5o7</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_115640-bdpwx5o7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: affwodpd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.009236221135107808\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_120152-affwodpd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/affwodpd' target=\"_blank\">crisp-sweep-21</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/affwodpd' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/affwodpd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▅▇█▆██▇</td></tr><tr><td>val_loss</td><td>█▆▂▃▂▁▃▁▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80676</td></tr><tr><td>train_loss</td><td>0.62541</td></tr><tr><td>val_accuracy</td><td>0.70861</td></tr><tr><td>val_loss</td><td>1.06765</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:128_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/affwodpd' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/affwodpd</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_120152-affwodpd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ychap5z3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008668925844398698\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_120704-ychap5z3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ychap5z3' target=\"_blank\">generous-sweep-22</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ychap5z3' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ychap5z3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇██▇█▇</td></tr><tr><td>val_loss</td><td>█▅▂▄▁▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.77849</td></tr><tr><td>train_loss</td><td>0.71876</td></tr><tr><td>val_accuracy</td><td>0.68748</td></tr><tr><td>val_loss</td><td>1.07398</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:2</strong> at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ychap5z3' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/ychap5z3</a><br> View project at: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_120704-ychap5z3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a3pgr49z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001590516685905841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_121633-a3pgr49z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/a3pgr49z' target=\"_blank\">visionary-sweep-23</a></strong> to <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/sweeps/olaafvbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/a3pgr49z' target=\"_blank\">https://wandb.ai/da24m015-iitm/dakshina-transliteration/runs/a3pgr49z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 291/1382 [00:11<00:41, 26.56it/s]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset utilities\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_vocab, output_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx]\n",
    "        input_ids = [self.input_vocab[c] for c in source]\n",
    "        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "def build_vocab(pairs):\n",
    "    input_chars = set()\n",
    "    output_chars = set()\n",
    "    for source, target in pairs:\n",
    "        input_chars.update(source)\n",
    "        output_chars.update(target)\n",
    "    input_vocab = {c: i + 1 for i, c in enumerate(sorted(input_chars))}\n",
    "    input_vocab['<pad>'] = 0\n",
    "    output_vocab = {c: i + 3 for i, c in enumerate(sorted(output_chars))}\n",
    "    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "def load_pairs(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"target\", \"source\", \"count\"], dtype=str)\n",
    "    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n",
    "    return list(zip(df[\"source\"], df[\"target\"]))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_lens = [len(seq) for seq in inputs]\n",
    "    target_lens = [len(seq) for seq in targets]\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, targets_padded, input_lens, target_lens\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs, hidden\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        timestep = encoder_outputs.size(1)\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, timestep, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy @ self.v\n",
    "        energy.masked_fill_(mask == 0, -1e10)\n",
    "        return torch.softmax(energy, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask):\n",
    "        embedded = self.embedding(input_token.unsqueeze(1))\n",
    "        attn_weights = self.attention(hidden[0] if isinstance(hidden, tuple) else hidden, encoder_outputs, mask)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)\n",
    "        rnn_input = torch.cat([embedded, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        output = self.fc(torch.cat([output.squeeze(1), context.squeeze(1)], dim=1))\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def create_mask(self, src):\n",
    "        return (src != 0).float()\n",
    "\n",
    "    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lens)\n",
    "        mask = self.create_mask(src).to(device)\n",
    "\n",
    "        if tgt is not None:\n",
    "            tgt_len = tgt.size(1)\n",
    "            outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features, device=device)\n",
    "            input_token = tgt[:, 0]\n",
    "            for t in range(1, tgt_len):\n",
    "                output, hidden = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "                outputs[:, t] = output\n",
    "                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "                input_token = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "            return outputs\n",
    "        else:\n",
    "            predictions = []\n",
    "            input_token = torch.tensor([1] * batch_size, device=device)  # <sos>\n",
    "            for _ in range(20):\n",
    "                output, hidden = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "                top1 = output.argmax(1)\n",
    "                predictions.append(top1.unsqueeze(1))\n",
    "                input_token = top1\n",
    "            return torch.cat(predictions, dim=1)\n",
    "\n",
    "def accuracy(preds, targets, pad_idx=0):\n",
    "    pred_tokens = preds.argmax(dim=-1)\n",
    "    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n",
    "    total = (targets != pad_idx).sum().item()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lens, tgt)\n",
    "        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        acc = accuracy(output[:, 1:], tgt[:, 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        output = model(src, src_lens, tgt, teacher_forcing_ratio=0.0)\n",
    "        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        acc = accuracy(output[:, 1:], tgt[:, 1:])\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "def main():\n",
    "    wandb.init(project=\"dakshina-transliteration\", config=wandb.config)\n",
    "    config = wandb.config\n",
    "\n",
    "    def generate_run_name(cfg):\n",
    "        return f\"attn_cell:{cfg.cell_type}_embed:{cfg.embed_size}_hid:{cfg.hidden_size}_layers:{cfg.num_layers}\"\n",
    "\n",
    "    wandb.run.name = generate_run_name(config)\n",
    "    wandb.run.save()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
    "    dev_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/hi/lexicons/hi.translit.sampled.dev.tsv\")\n",
    "    input_vocab, output_vocab = build_vocab(train_pairs)\n",
    "    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n",
    "    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    encoder = Encoder(len(input_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n",
    "    decoder = Decoder(len(output_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_accuracy\": train_acc, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sweep_config = {\n",
    "        \"method\": \"bayes\",\n",
    "        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"embed_size\": {\"values\": [64, 128]},\n",
    "            \"hidden_size\": {\"values\": [64, 128]},\n",
    "            \"num_layers\": {\"values\": [1, 2]},\n",
    "            \"cell_type\": {\"values\": [\"GRU\", \"LSTM\"]},\n",
    "            \"dropout\": {\"values\": [0.2, 0.3]},\n",
    "            \"lr\": {\"min\": 0.0001, \"max\": 0.01},\n",
    "            \"batch_size\": {\"values\": [32, 64]}\n",
    "        }\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration\")\n",
    "    wandb.agent(sweep_id, function=main, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T13:33:21.251673Z",
     "iopub.status.busy": "2025-05-19T13:33:21.251205Z",
     "iopub.status.idle": "2025-05-19T13:49:52.577750Z",
     "shell.execute_reply": "2025-05-19T13:49:52.577088Z",
     "shell.execute_reply.started": "2025-05-19T13:33:21.251649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 1.3512\n",
      "\n",
      "Test Accuracy: 25.88%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगराक               | Truth: अंगारक\n",
      "Epoch 2 Train Loss: 0.7973\n",
      "\n",
      "Test Accuracy: 29.30%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकर                | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगररक               | Truth: अंगारक\n",
      "Epoch 3 Train Loss: 0.6850\n",
      "\n",
      "Test Accuracy: 33.14%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 4 Train Loss: 0.6131\n",
      "\n",
      "Test Accuracy: 34.34%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 5 Train Loss: 0.5647\n",
      "\n",
      "Test Accuracy: 31.83%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अनका                 | Truth: अंक\n",
      "ankit           | Pred: अनकित                | Truth: अंकित\n",
      "anakon          | Pred: अनाकों               | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अनकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अनकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगराक               | Truth: अंगारक\n",
      "Epoch 6 Train Loss: 0.5275\n",
      "\n",
      "Test Accuracy: 34.03%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंकोर                | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 7 Train Loss: 0.5031\n",
      "\n",
      "Test Accuracy: 35.56%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अनकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: आंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अनकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगराक               | Truth: अंगारक\n",
      "Epoch 8 Train Loss: 0.4639\n",
      "\n",
      "Test Accuracy: 34.38%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: एंककर                | Truth: अंकोर\n",
      "ankor           | Pred: अनकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगरक                | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 9 Train Loss: 0.4529\n",
      "\n",
      "Test Accuracy: 31.39%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: आनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: आंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंकोर                | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 10 Train Loss: 0.4281\n",
      "\n",
      "Test Accuracy: 33.12%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: आंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकीत                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखोंं               | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: एंकॉर                | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "\n",
      "Loading best model for final evaluation...\n",
      "\n",
      "Test Accuracy: 35.56%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अनकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: आंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अनकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगराक               | Truth: अंगारक\n",
      "\n",
      "Predictions saved to: test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# ---------------- Dataset & Utils ----------------\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_vocab, output_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx]\n",
    "        input_ids = [self.input_vocab[c] for c in source]\n",
    "        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "def load_pairs(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n",
    "    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n",
    "    return list(zip(df['source'], df['target']))\n",
    "\n",
    "def build_vocab(pairs):\n",
    "    input_chars = set()\n",
    "    output_chars = set()\n",
    "    for src, tgt in pairs:\n",
    "        input_chars.update(src)\n",
    "        output_chars.update(tgt)\n",
    "    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n",
    "    input_vocab['<pad>'] = 0\n",
    "    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n",
    "    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_lens = [len(x) for x in inputs]\n",
    "    target_lens = [len(x) for x in targets]\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, targets_padded, input_lens, target_lens\n",
    "\n",
    "# ---------------- Bahdanau Attention ----------------\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, attn_size)\n",
    "        self.W2 = nn.Linear(hidden_size, attn_size)\n",
    "        self.V = nn.Linear(attn_size, 1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size) or (batch, hidden_size)\n",
    "        # encoder_outputs: (batch, seq_len, hidden_size)\n",
    "        # We'll take hidden from last layer (batch, hidden_size)\n",
    "        if hidden.dim() == 3:\n",
    "            hidden = hidden[-1]  # take last layer, shape: (batch, hidden_size)\n",
    "        hidden_with_time_axis = hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n",
    "        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)))  # (batch, seq_len, 1)\n",
    "        attn_weights = torch.softmax(score, dim=1)  # (batch, seq_len, 1)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights * mask.unsqueeze(2)  # apply mask\n",
    "            attn_weights = attn_weights / (attn_weights.sum(dim=1, keepdim=True) + 1e-10)\n",
    "        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)  # (batch, hidden_size)\n",
    "        return context_vector, attn_weights.squeeze(-1)  # attn_weights shape (batch, seq_len)\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=False)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)  # (batch, seq_len, hidden_size)\n",
    "        return outputs, hidden  # outputs for attention, hidden for decoder init\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout, attn_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.attention = BahdanauAttention(hidden_size, attn_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # concat context vector + rnn output\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n",
    "        # input_token: (batch,), hidden: (num_layers, batch, hidden_size)\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (batch, 1, embed_size)\n",
    "        context_vector, attn_weights = self.attention(hidden, encoder_outputs, mask)  # (batch, hidden_size), (batch, seq_len)\n",
    "        rnn_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=-1)  # (batch, 1, embed+hidden)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # output: (batch,1,hidden_size)\n",
    "        output = output.squeeze(1)  # (batch, hidden_size)\n",
    "        output = torch.cat((output, context_vector), dim=1)  # (batch, hidden_size*2)\n",
    "        output = self.fc(output)  # (batch, output_size)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_vocab):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def create_mask(self, src, src_lens):\n",
    "        # mask for padding tokens in encoder outputs (batch, seq_len)\n",
    "        batch_size, seq_len = src.size()\n",
    "        mask = torch.arange(seq_len).expand(batch_size, seq_len).to(src.device) < torch.tensor(src_lens).unsqueeze(1).to(src.device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5, max_len=20):\n",
    "        batch_size = src.size(0)\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lens)  # encoder_outputs (batch, seq_len, hidden), hidden (num_layers, batch, hidden)\n",
    "        mask = self.create_mask(src, src_lens)\n",
    "\n",
    "        # Initialize decoder input and outputs\n",
    "        if tgt is not None:\n",
    "            tgt_len = tgt.size(1)\n",
    "        else:\n",
    "            tgt_len = max_len\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n",
    "        input_token = torch.tensor([self.sos] * batch_size).to(src.device)\n",
    "\n",
    "        # For LSTM hidden is tuple (h,c), for others just tensor\n",
    "        decoder_hidden = hidden\n",
    "        if isinstance(hidden, tuple):\n",
    "            decoder_hidden = (hidden[0], hidden[1])  # just keep as is\n",
    "\n",
    "        for t in range(tgt_len):\n",
    "            output, decoder_hidden, attn_weights = self.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = tgt is not None and (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            if teacher_force and t + 1 < tgt_len:\n",
    "                input_token = tgt[:, t+1]\n",
    "            else:\n",
    "                input_token = output.argmax(1)\n",
    "        return outputs\n",
    "\n",
    "# ---------------- Train + Eval ----------------\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt, src_lens, _ in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lens, tgt)\n",
    "        loss = criterion(output[:, :-1].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_and_save(model, dataloader, input_vocab, output_vocab, device, csv_path=None):\n",
    "    model.eval()\n",
    "    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n",
    "    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_lens, _ in dataloader:\n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            encoder_outputs, hidden = model.encoder(src, src_lens)\n",
    "            mask = model.create_mask(src, src_lens)\n",
    "            input_token = torch.tensor([output_vocab['<sos>']] * batch_size).to(device)\n",
    "            decoder_hidden = hidden\n",
    "            decoded_tokens = []\n",
    "\n",
    "            max_len = 20\n",
    "            for _ in range(max_len):\n",
    "                output, decoder_hidden, attn_weights = model.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "                input_token = output.argmax(1)\n",
    "                decoded_tokens.append(input_token.unsqueeze(1))\n",
    "            decoded = torch.cat(decoded_tokens, dim=1)  # (batch, max_len)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred = ''.join([inv_output_vocab[t.item()] for t in decoded[i] if t.item() not in [output_vocab['<eos>'], 0]])\n",
    "                truth = ''.join([inv_output_vocab[t.item()] for t in tgt[i][1:-1]])\n",
    "                inp = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n",
    "                results.append((inp, pred, truth))\n",
    "                if pred == truth:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    acc = correct / total * 100 if total > 0 else 0\n",
    "    print(f\"\\nTest Accuracy: {acc:.2f}%\")\n",
    "    for inp, pred, truth in results[:10]:\n",
    "        print(f\"{inp:<15} | Pred: {pred:<20} | Truth: {truth}\")\n",
    "\n",
    "    if csv_path is not None:\n",
    "        with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n",
    "            writer.writerows(results)\n",
    "        print(f\"\\nPredictions saved to: {csv_path}\")\n",
    "\n",
    "    return acc, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"embed_size\": 128,\n",
    "        \"hidden_size\": 128,\n",
    "        \"attn_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 0.001590516685905841,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
    "    test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/hi/lexicons/hi.translit.sampled.test.tsv\")\n",
    "    input_vocab, output_vocab = build_vocab(train_pairs)\n",
    "    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n",
    "    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n",
    "    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"], config[\"attn_size\"])\n",
    "    model = Seq2Seq(encoder, decoder, output_vocab).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n",
    "        acc, results = evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=None)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(\"\\nLoading best model for final evaluation...\")\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T13:53:02.055265Z",
     "iopub.status.busy": "2025-05-19T13:53:02.054548Z",
     "iopub.status.idle": "2025-05-19T13:53:06.131620Z",
     "shell.execute_reply": "2025-05-19T13:53:06.130977Z",
     "shell.execute_reply.started": "2025-05-19T13:53:02.055240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating attention heatmaps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/3041608184.py:258: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated attention heatmaps for 9 examples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# ---------------- Dataset & Utils ----------------\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_vocab, output_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx]\n",
    "        input_ids = [self.input_vocab[c] for c in source]\n",
    "        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "def load_pairs(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n",
    "    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n",
    "    return list(zip(df['source'], df['target']))\n",
    "\n",
    "def build_vocab(pairs):\n",
    "    input_chars = set()\n",
    "    output_chars = set()\n",
    "    for src, tgt in pairs:\n",
    "        input_chars.update(src)\n",
    "        output_chars.update(tgt)\n",
    "    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n",
    "    input_vocab['<pad>'] = 0\n",
    "    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n",
    "    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_lens = [len(x) for x in inputs]\n",
    "    target_lens = [len(x) for x in targets]\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, targets_padded, input_lens, target_lens\n",
    "\n",
    "# ---------------- Bahdanau Attention ----------------\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, attn_size)\n",
    "        self.W2 = nn.Linear(hidden_size, attn_size)\n",
    "        self.V = nn.Linear(attn_size, 1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size) or (batch, hidden_size)\n",
    "        # encoder_outputs: (batch, seq_len, hidden_size)\n",
    "        # We'll take hidden from last layer (batch, hidden_size)\n",
    "        if hidden.dim() == 3:\n",
    "            hidden = hidden[-1]  # take last layer, shape: (batch, hidden_size)\n",
    "        hidden_with_time_axis = hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n",
    "        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)))  # (batch, seq_len, 1)\n",
    "        attn_weights = torch.softmax(score, dim=1)  # (batch, seq_len, 1)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights * mask.unsqueeze(2)  # apply mask\n",
    "            attn_weights = attn_weights / (attn_weights.sum(dim=1, keepdim=True) + 1e-10)\n",
    "        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)  # (batch, hidden_size)\n",
    "        return context_vector, attn_weights.squeeze(-1)  # attn_weights shape (batch, seq_len)\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=False)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)  # (batch, seq_len, hidden_size)\n",
    "        return outputs, hidden  # outputs for attention, hidden for decoder init\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout, attn_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.attention = BahdanauAttention(hidden_size, attn_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # concat context vector + rnn output\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n",
    "        # input_token: (batch,), hidden: (num_layers, batch, hidden_size)\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (batch, 1, embed_size)\n",
    "        context_vector, attn_weights = self.attention(hidden, encoder_outputs, mask)  # (batch, hidden_size), (batch, seq_len)\n",
    "        rnn_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=-1)  # (batch, 1, embed+hidden)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # output: (batch,1,hidden_size)\n",
    "        output = output.squeeze(1)  # (batch, hidden_size)\n",
    "        output = torch.cat((output, context_vector), dim=1)  # (batch, hidden_size*2)\n",
    "        output = self.fc(output)  # (batch, output_size)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_vocab):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def create_mask(self, src, src_lens):\n",
    "        # mask for padding tokens in encoder outputs (batch, seq_len)\n",
    "        batch_size, seq_len = src.size()\n",
    "        mask = torch.arange(seq_len).expand(batch_size, seq_len).to(src.device) < torch.tensor(src_lens).unsqueeze(1).to(src.device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5, max_len=20):\n",
    "        batch_size = src.size(0)\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lens)  # encoder_outputs (batch, seq_len, hidden), hidden (num_layers, batch, hidden)\n",
    "        mask = self.create_mask(src, src_lens)\n",
    "\n",
    "        # Initialize decoder input and outputs\n",
    "        if tgt is not None:\n",
    "            tgt_len = tgt.size(1)\n",
    "        else:\n",
    "            tgt_len = max_len\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n",
    "        input_token = torch.tensor([self.sos] * batch_size).to(src.device)\n",
    "\n",
    "        # For LSTM hidden is tuple (h,c), for others just tensor\n",
    "        decoder_hidden = hidden\n",
    "        if isinstance(hidden, tuple):\n",
    "            decoder_hidden = (hidden[0], hidden[1])  # just keep as is\n",
    "\n",
    "        for t in range(tgt_len):\n",
    "            output, decoder_hidden, attn_weights = self.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = tgt is not None and (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            if teacher_force and t + 1 < tgt_len:\n",
    "                input_token = tgt[:, t+1]\n",
    "            else:\n",
    "                input_token = output.argmax(1)\n",
    "        return outputs\n",
    "\n",
    "# ---------------- New Attention Heatmap Visualization ----------------\n",
    "def generate_attention_heatmap(model, test_loader, input_vocab, output_vocab, device, num_examples=9):\n",
    "    model.eval()\n",
    "    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n",
    "    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_lens, _ in test_loader:\n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            encoder_outputs, hidden = model.encoder(src, src_lens)\n",
    "            mask = model.create_mask(src, src_lens)\n",
    "            \n",
    "            # Get input text\n",
    "            input_text = ''.join([inv_input_vocab[t.item()] for t in src[0] if t.item() != 0])\n",
    "            \n",
    "            # Get target text\n",
    "            target_text = ''.join([inv_output_vocab[t.item()] for t in tgt[0][1:-1]])\n",
    "            \n",
    "            # Initialize for decoding\n",
    "            input_token = torch.tensor([output_vocab['<sos>']]).to(device)\n",
    "            decoder_hidden = hidden\n",
    "            \n",
    "            # Store attention weights and predicted tokens\n",
    "            attention_weights = []\n",
    "            predicted_text = []\n",
    "            \n",
    "            # Decode\n",
    "            max_len = 20\n",
    "            for _ in range(max_len):\n",
    "                output, decoder_hidden, attn_weights = model.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "                attention_weights.append(attn_weights[0].cpu().numpy())\n",
    "                \n",
    "                input_token = output.argmax(1)\n",
    "                pred_token = inv_output_vocab.get(input_token.item(), '<unk>')\n",
    "                \n",
    "                if pred_token == '<eos>' or len(predicted_text) >= max_len:\n",
    "                    break\n",
    "                    \n",
    "                if pred_token not in ['<sos>', '<pad>']:\n",
    "                    predicted_text.append(pred_token)\n",
    "            \n",
    "            predicted_text = ''.join(predicted_text)\n",
    "            attention_matrix = np.array(attention_weights)\n",
    "            \n",
    "            # Add to examples\n",
    "            examples.append({\n",
    "                'input': input_text,\n",
    "                'target': target_text,\n",
    "                'prediction': predicted_text,\n",
    "                'attention': attention_matrix[:len(predicted_text), :len(input_text)]\n",
    "            })\n",
    "    \n",
    "    # Plot attention heatmaps in a grid\n",
    "    if examples:\n",
    "        num_examples = min(9, len(examples))\n",
    "        rows, cols = 3, 3  # Fixed 3x3 grid\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 18))\n",
    "        \n",
    "        for i, example in enumerate(examples[:num_examples]):\n",
    "            if i >= rows * cols:\n",
    "                break\n",
    "                \n",
    "            # Create subplot with more space for title\n",
    "            ax = fig.add_subplot(rows, cols, i + 1)\n",
    "            \n",
    "            # Add title with input, target and prediction\n",
    "            title = f\"Input: {example['input']}\\nTarget: {example['target']}\\nPred: {example['prediction']}\"\n",
    "            ax.set_title(title, fontsize=12, pad=10)\n",
    "            \n",
    "            # Generate attention heatmap\n",
    "            attention_data = example['attention']\n",
    "            \n",
    "            # Create heatmap with improved appearance\n",
    "            sns.heatmap(\n",
    "                attention_data, \n",
    "                ax=ax,\n",
    "                xticklabels=list(example['input']),\n",
    "                yticklabels=['' for _ in range(len(example['prediction']))],  # Empty y-labels like in your example\n",
    "                cmap='viridis',  # Similar to your example\n",
    "                vmin=0.0,        # Minimum value\n",
    "                vmax=1.0,        # Maximum value\n",
    "                square=True,     # Square cells\n",
    "                cbar_kws={'shrink': 0.8}  # Colorbar settings\n",
    "            )\n",
    "            \n",
    "            # Enhance the appearance of the axis labels\n",
    "            plt.setp(ax.get_xticklabels(), fontsize=12, rotation=0)\n",
    "            plt.setp(ax.get_yticklabels(), fontsize=12, rotation=0)\n",
    "            \n",
    "            # Add colorbar ticks\n",
    "            colorbar = ax.collections[0].colorbar\n",
    "            colorbar.set_ticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "            colorbar.set_ticklabels(['0.0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8'])\n",
    "                \n",
    "        plt.tight_layout(pad=3.0)  # Add padding between subplots\n",
    "        plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Generated attention heatmaps for {num_examples} examples\")\n",
    "        return examples\n",
    "    else:\n",
    "        print(\"No examples found\")\n",
    "        return []\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"embed_size\": 128,\n",
    "        \"hidden_size\": 128,\n",
    "        \"attn_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 0.001590516685905841,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
    "    test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/hi/lexicons/hi.translit.sampled.test.tsv\")\n",
    "    input_vocab, output_vocab = build_vocab(train_pairs)\n",
    "    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n",
    "    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n",
    "    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"], config[\"attn_size\"])\n",
    "    model = Seq2Seq(encoder, decoder, output_vocab).to(device)\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    \n",
    "    # Generate attention heatmaps\n",
    "    print(\"Generating attention heatmaps...\")\n",
    "    examples = generate_attention_heatmap(model, test_loader, input_vocab, output_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7381253,
     "sourceId": 11757835,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
